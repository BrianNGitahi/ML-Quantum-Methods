{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWy_Koj-t5CZ",
        "outputId": "489cf5cc-714e-4096-dab5-43ca16acbdff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empirical distribution from circuit (first 10 rows):\n",
            "[[1 1 0 1]\n",
            " [1 1 1 0]\n",
            " [1 0 0 0]\n",
            " [0 0 1 1]\n",
            " [1 1 0 1]\n",
            " [0 0 1 1]\n",
            " [1 0 1 1]\n",
            " [1 0 1 0]\n",
            " [1 1 1 0]\n",
            " [0 0 1 1]]\n",
            "Epoch  150  PLL≈ -2.7761\n",
            "Epoch  300  PLL≈ -2.7625\n",
            "Epoch  450  PLL≈ -2.7732\n",
            "Epoch  600  PLL≈ -2.7745\n",
            "Epoch  750  PLL≈ -2.7790\n",
            "Epoch  900  PLL≈ -2.7716\n",
            "Epoch 1050  PLL≈ -2.7687\n",
            "Epoch 1200  PLL≈ -2.7794\n",
            "Epoch 1350  PLL≈ -2.7743\n",
            "Epoch 1500  PLL≈ -2.7753\n",
            "\n",
            "--- Evaluation ---\n",
            "True probs (first 8):   [0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625]\n",
            "Model probs (first 8):  [0.0597 0.061  0.062  0.0619 0.0605 0.0609 0.0662 0.0626]\n",
            "TVD(true, model) = 0.0111\n",
            "KL(true || model) = 0.0004\n"
          ]
        }
      ],
      "source": [
        "# rbm_train_4q_test.py\n",
        "# Minimal, dependency-free (NumPy-only) test of training an RBM on bitstrings\n",
        "# sampled from a 4-qubit quantum circuit.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "def kron(*ops):\n",
        "    out = np.array([[1.0+0j]])\n",
        "    for op in ops:\n",
        "        out = np.kron(out, op)\n",
        "    return out\n",
        "\n",
        "def normalize(psi):\n",
        "    return psi / np.linalg.norm(psi)\n",
        "\n",
        "def sample_bitstrings_from_state(psi, n_samples, rng):\n",
        "    # psi is a 16-dim statevector for 4 qubits (|0000>..|1111> order)\n",
        "    probs = np.abs(psi)**2\n",
        "    idxs = rng.choice(len(probs), size=n_samples, p=probs)\n",
        "    # Convert indices to 4-bit strings (0/1)\n",
        "    bits = ((idxs[:, None] >> np.arange(4)[::-1]) & 1).astype(np.int64)\n",
        "    return bits\n",
        "\n",
        "def one_hot_counts(bits, n_qubits=4):\n",
        "    idx = np.packbits(bits, axis=1, bitorder='big')  # 4 bits -> 1 byte\n",
        "    # But packbits returns uint8 with all 8 bits; compute integer index manually:\n",
        "    idx = (bits[:,0]<<3) + (bits[:,1]<<2) + (bits[:,2]<<1) + (bits[:,3]<<0)\n",
        "    counts = np.bincount(idx, minlength=2**n_qubits)\n",
        "    return counts\n",
        "\n",
        "def total_variation_distance(p, q):\n",
        "    return 0.5 * np.sum(np.abs(p - q))\n",
        "\n",
        "# ---------- Simple 4-qubit circuit (statevector) ----------\n",
        "def four_qubit_state(rng=None):\n",
        "    # Gates\n",
        "    I = np.eye(2, dtype=complex)\n",
        "    X = np.array([[0,1],[1,0]], dtype=complex)\n",
        "    H = (1/np.sqrt(2))*np.array([[1,1],[1,-1]], dtype=complex)\n",
        "    Rz = lambda theta: np.array([[np.exp(-1j*theta/2),0],[0,np.exp(1j*theta/2)]], dtype=complex)\n",
        "    Rx = lambda theta: np.array([[np.cos(theta/2), -1j*np.sin(theta/2)],\n",
        "                                 [-1j*np.sin(theta/2), np.cos(theta/2)]], dtype=complex)\n",
        "    # CNOT on control c, target t; 4-qubit space\n",
        "    def CNOT(c, t):\n",
        "        # Build 16x16 operator by projecting on control\n",
        "        P0 = np.array([[1,0],[0,0]], dtype=complex)\n",
        "        P1 = np.array([[0,0],[0,1]], dtype=complex)\n",
        "        ops0 = [I,I,I,I]; ops1 = [I,I,I,I]\n",
        "        ops0[c] = P0; ops1[c] = P1\n",
        "        U0 = kron(*ops0)\n",
        "        # X on target when control is 1\n",
        "        opsx = [I,I,I,I]\n",
        "        opsx[t] = X\n",
        "        X_t = kron(*opsx)\n",
        "        return U0 + kron(*[P1 if i==c else I for i in range(4)]) @ X_t\n",
        "\n",
        "    # Start in |0000>\n",
        "    psi = np.zeros(16, dtype=complex); psi[0] = 1.0\n",
        "\n",
        "    # Layer 1: H on all qubits\n",
        "    U_H_all = kron(H,H,H,H)\n",
        "    psi = U_H_all @ psi\n",
        "\n",
        "    # Layer 2: random single-qubit rotations (fixed seed for reproducibility)\n",
        "    if rng is None: rng = np.random.default_rng(7)\n",
        "    thetas_rz = rng.uniform(-0.9, 0.9, size=4)\n",
        "    thetas_rx = rng.uniform(-0.9, 0.9, size=4)\n",
        "    U1 = kron(Rz(thetas_rz[0])@Rx(thetas_rx[0]),\n",
        "              Rz(thetas_rz[1])@Rx(thetas_rx[1]),\n",
        "              Rz(thetas_rz[2])@Rx(thetas_rx[2]),\n",
        "              Rz(thetas_rz[3])@Rx(thetas_rx[3]))\n",
        "    psi = U1 @ psi\n",
        "\n",
        "    # Layer 3: entanglers (CNOT 0->1 and 2->3)\n",
        "    psi = CNOT(0,1) @ psi\n",
        "    psi = CNOT(2,3) @ psi\n",
        "\n",
        "    return normalize(psi)\n",
        "\n",
        "# ---------- RBM (0/1 units) ----------\n",
        "class RBM01:\n",
        "    \"\"\"\n",
        "    RBM with 0/1 visible and hidden units.\n",
        "    Energy:  E(v,h) = -a^T v - b^T h - v^T W h\n",
        "    p(v,h) ∝ exp(-E). Conditionals:\n",
        "      p(h=1|v) = sigmoid(b + W^T v)\n",
        "      p(v=1|h) = sigmoid(a + W h)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_visible, n_hidden, rng=None, scale=0.01):\n",
        "        self.nv = n_visible\n",
        "        self.nh = n_hidden\n",
        "        self.rng = np.random.default_rng() if rng is None else rng\n",
        "        self.a = scale*self.rng.standard_normal(self.nv)\n",
        "        self.b = scale*self.rng.standard_normal(self.nh)\n",
        "        self.W = scale*self.rng.standard_normal((self.nv, self.nh))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "    def p_h_given_v(self, v):\n",
        "        # v: (B, nv) or (nv,)\n",
        "        x = self.b + v @ self.W\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "    def p_v_given_h(self, h):\n",
        "        x = self.a + h @ self.W.T\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        ph = self.p_h_given_v(v)\n",
        "        return (self.rng.random(ph.shape) < ph).astype(np.float64), ph\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        pv = self.p_v_given_h(h)\n",
        "        return (self.rng.random(pv.shape) < pv).astype(np.float64), pv\n",
        "\n",
        "    def cd_k(self, v0, k=1):\n",
        "        v = v0.copy()\n",
        "        h, ph = self.sample_h(v)\n",
        "        for _ in range(k):\n",
        "            v, pv = self.sample_v(h)\n",
        "            h, ph = self.sample_h(v)\n",
        "        return v, h  # vk, hk\n",
        "\n",
        "    def train(self, data, epochs=2000, batch_size=128, lr=0.05, k=1, verbose_every=200):\n",
        "        n = data.shape[0]\n",
        "        for ep in range(1, epochs+1):\n",
        "            # mini-batch SGD\n",
        "            perm = self.rng.permutation(n)\n",
        "            for i in range(0, n, batch_size):\n",
        "                batch = data[perm[i:i+batch_size]]\n",
        "                # Positive phase\n",
        "                ph_pos = self.p_h_given_v(batch)\n",
        "                # Negative phase via CD-k\n",
        "                v_k, h_k = self.cd_k(batch, k=k)\n",
        "                ph_neg = self.p_h_given_v(v_k)\n",
        "\n",
        "                # Gradients (expected sufficient statistics diff)\n",
        "                dW = (batch.T @ ph_pos - v_k.T @ ph_neg) / batch.shape[0]\n",
        "                da = (batch - v_k).mean(axis=0)\n",
        "                db = (ph_pos - ph_neg).mean(axis=0)\n",
        "\n",
        "                # Update\n",
        "                self.W += lr * dW\n",
        "                self.a += lr * da\n",
        "                self.b += lr * db\n",
        "\n",
        "            if verbose_every and ep % verbose_every == 0:\n",
        "                pll = self.pseudo_log_likelihood(data[:256])\n",
        "                print(f\"Epoch {ep:4d}  PLL≈ {pll:.4f}\")\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        # F(v) = -a^T v - sum_j log(1 + exp(b_j + W_j^T v))\n",
        "        lin = self.a @ v.T  # shape: (B,)\n",
        "        t = self.b + v @ self.W\n",
        "        logsum = np.sum(np.log1p(np.exp(t)), axis=1)\n",
        "        return -lin - logsum\n",
        "\n",
        "    def pseudo_log_likelihood(self, data):\n",
        "        # Standard RBM diagnostic: average PLL via single bit flip\n",
        "        B = min(256, data.shape[0])\n",
        "        idx = self.rng.integers(0, self.nv, size=B)\n",
        "        v = data[:B].copy()\n",
        "        fe_v = self.free_energy(v)\n",
        "        # flip selected bit\n",
        "        v_flipped = v.copy()\n",
        "        rows = np.arange(B)\n",
        "        v_flipped[rows, idx] = 1.0 - v_flipped[rows, idx]\n",
        "        fe_vf = self.free_energy(v_flipped)\n",
        "        pll = self.nv * np.mean(np.log(self.sigmoid(fe_vf - fe_v)))\n",
        "        return pll\n",
        "\n",
        "    def sample_model(self, n_samples=10000, k=20):\n",
        "        # Start from random visibles; run k-step block Gibbs; return samples\n",
        "        v = (self.rng.random((n_samples, self.nv)) < 0.5).astype(np.float64)\n",
        "        for _ in range(k):\n",
        "            h, _ = self.sample_h(v)\n",
        "            v, _ = self.sample_v(h)\n",
        "        return v.astype(int)\n",
        "\n",
        "# ---------- Main test ----------\n",
        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(123)\n",
        "\n",
        "    # 1) Build 4-qubit state and sample training data\n",
        "    psi = four_qubit_state(rng)\n",
        "    n_train = 20000\n",
        "    data_bits = sample_bitstrings_from_state(psi, n_train, rng)  # shape (n,4), 0/1\n",
        "    print(\"Empirical distribution from circuit (first 10 rows):\")\n",
        "    print(data_bits[:10])\n",
        "\n",
        "    # True distribution (for evaluation)\n",
        "    true_probs = np.abs(psi)**2  # length 16\n",
        "\n",
        "    # 2) Initialize and train RBM\n",
        "    rbm = RBM01(n_visible=4, n_hidden=8, rng=rng)\n",
        "    rbm.train(data_bits, epochs=1500, batch_size=128, lr=0.05, k=1, verbose_every=150)\n",
        "\n",
        "    # 3) Evaluate: sample from RBM model and compare to true probs\n",
        "    rbm_samples = rbm.sample_model(n_samples=200000, k=25)\n",
        "    model_counts = one_hot_counts(rbm_samples, n_qubits=4).astype(np.float64)\n",
        "    model_probs = model_counts / model_counts.sum()\n",
        "\n",
        "    tvd = total_variation_distance(true_probs, model_probs)\n",
        "    kl = np.sum(np.where(model_probs > 0, true_probs * (np.log(true_probs + 1e-12) - np.log(model_probs + 1e-12)), 0.0))\n",
        "    print(\"\\n--- Evaluation ---\")\n",
        "    print(\"True probs (first 8):  \", np.round(true_probs[:8], 4))\n",
        "    print(\"Model probs (first 8): \", np.round(model_probs[:8], 4))\n",
        "    print(f\"TVD(true, model) = {tvd:.4f}\")\n",
        "    print(f\"KL(true || model) = {kl:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1+1j)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1+1j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg9msoa_xHs0",
        "outputId": "4284f495-4ce8-4b5f-ace9-f32d4712939e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MC estimate  <O> = -0.023905+0.000677j  ± 0.005717  (standard error)\n",
            "Exact value  <O> = -0.018401+0.000000j\n",
            "Absolute error |MC - Exact| = 5.546242e-03\n"
          ]
        }
      ],
      "source": [
        "# rbm_observable_estimator_test.py\n",
        "# Estimate <O> for O = sum_k c_k P_k using a trained complex RBM psi_lambda.\n",
        "# Demonstrates MC local-estimator + exact enumeration cross-check (small N).\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------\n",
        "# Complex RBM wavefunction\n",
        "# ----------------------------\n",
        "class ComplexRBM:\n",
        "    \"\"\"\n",
        "    RBM with spins sigma_i in {-1, +1}.\n",
        "    psi(sigma) = exp(sum_i a_i sigma_i) * prod_j 2 cosh( b_j + sum_i W_ij sigma_i )\n",
        "    where a, b, W are complex.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_visible, n_hidden, a=None, b=None, W=None, rng=None):\n",
        "        self.N = n_visible\n",
        "        self.M = n_hidden\n",
        "        self.rng = np.random.default_rng() if rng is None else rng\n",
        "\n",
        "        # If not supplied, set arbitrary (random) complex params for demo\n",
        "        if a is None: a = 0.05*(self.rng.standard_normal(self.N) + 1j*self.rng.standard_normal(self.N))\n",
        "        if b is None: b = 0.05*(self.rng.standard_normal(self.M) + 1j*self.rng.standard_normal(self.M))\n",
        "        if W is None: W = 0.05*(self.rng.standard_normal((self.N,self.M)) + 1j*self.rng.standard_normal((self.N,self.M)))\n",
        "\n",
        "        self.a = a.astype(np.complex128)\n",
        "        self.b = b.astype(np.complex128)\n",
        "        self.W = W.astype(np.complex128)\n",
        "\n",
        "    def theta(self, sigma):\n",
        "        # theta_j = b_j + sum_i W_ij sigma_i ; shape (M,)\n",
        "        return self.b + (sigma @ self.W)\n",
        "\n",
        "    def logpsi(self, sigma):\n",
        "        # sigma: shape (N,) with entries in {-1, +1}\n",
        "        th = self.theta(sigma)  # (M,)\n",
        "        # log psi = sum_i a_i sigma_i + sum_j log(2 cosh(theta_j))\n",
        "        # Use np.cosh for complex; add small epsilon to avoid branch issues if desired\n",
        "        return np.sum(self.a * sigma) + np.sum(np.log(2.0*np.cosh(th)))\n",
        "\n",
        "    def psi(self, sigma):\n",
        "        return np.exp(self.logpsi(sigma))\n",
        "\n",
        "    # ---- Metropolis sampling from |psi|^2 ----\n",
        "    def metropolis_samples(self, n_samples=10000, burn_in=1000, thin=10):\n",
        "        sigma = self.random_spin_config()  # start\n",
        "        logpsi = self.logpsi(sigma)\n",
        "        # cache theta for O(M) updates\n",
        "        th = self.theta(sigma)\n",
        "\n",
        "        samples = []\n",
        "        total_steps = burn_in + n_samples*thin\n",
        "        for step in range(total_steps):\n",
        "            i = self.rng.integers(self.N)           # propose flip at i\n",
        "            sigma_new = sigma.copy()\n",
        "            sigma_new[i] *= -1\n",
        "\n",
        "            # Efficient theta' update: theta'_j = theta_j + W_ij (sigma'_i - sigma_i) = theta_j - 2 W_ij sigma_i\n",
        "            th_new = th - 2.0*self.W[i,:]*sigma[i]\n",
        "\n",
        "            # log |psi|^2 = 2 * Re(logpsi)\n",
        "            logpsi2 = 2.0*np.real(logpsi)\n",
        "            logpsi_new = (np.sum(self.a * sigma_new) + np.sum(np.log(2.0*np.cosh(th_new))))\n",
        "            logpsi2_new = 2.0*np.real(logpsi_new)\n",
        "\n",
        "            # Accept/reject\n",
        "            logR = logpsi2_new - logpsi2\n",
        "            if logR >= 0 or np.log(self.rng.random()) < logR:\n",
        "                sigma = sigma_new\n",
        "                logpsi = logpsi_new\n",
        "                th = th_new\n",
        "\n",
        "            # collect\n",
        "            if step >= burn_in and ((step - burn_in) % thin == 0):\n",
        "                samples.append(sigma.copy())\n",
        "\n",
        "        return np.array(samples, dtype=int)\n",
        "\n",
        "    def random_spin_config(self):\n",
        "        return 2*self.rng.integers(0,2,size=self.N)-1  # {-1,+1}\n",
        "\n",
        "# -------------------------------------------\n",
        "# Pauli words: apply in Z-basis & phase rules\n",
        "# -------------------------------------------\n",
        "# Represent a Pauli word as a list of ('I'/'X'/'Y'/'Z') of length N.\n",
        "# Local estimator for a single sample sigma uses:\n",
        "#   <sigma | P | psi> / <sigma | psi> = phase * psi(sigma_flipped)/psi(sigma)\n",
        "# where sigma_flipped flips bits where X or Y act; phase accumulates Z and Y phases.\n",
        "\n",
        "def apply_pauli_on_config(sigma, pauli_word):\n",
        "    \"\"\"\n",
        "    Returns (sigma_prime, phase) such that\n",
        "    <sigma | P | psi> = phase * psi(sigma_prime)\n",
        "    in the computational Z basis with sigma_i in {-1, +1} mapping to |0>, |1> by sigma_i=+1->|0|, -1->|1|.\n",
        "    Phase convention:\n",
        "      Z on |0>,|1> gives +1,-1 respectively => factor sigma_i\n",
        "      X flips the bit (no phase)\n",
        "      Y flips the bit and contributes phase i*(sign) where sign = -sigma_i (since Y|0>=i|1>, Y|1>=-i|0>)\n",
        "    \"\"\"\n",
        "    sigma_prime = sigma.copy()\n",
        "    phase = 1.0 + 0.0j\n",
        "    for i, P in enumerate(pauli_word):\n",
        "        if P == 'I':\n",
        "            continue\n",
        "        elif P == 'Z':\n",
        "            phase *= sigma[i]  # eigenvalue +1/-1\n",
        "        elif P == 'X':\n",
        "            sigma_prime[i] *= -1\n",
        "        elif P == 'Y':\n",
        "            # Y = i X Z ; acting on basis adds: flip and multiply by (i * sigma_i)\n",
        "            phase *= (1j * sigma[i])\n",
        "            sigma_prime[i] *= -1\n",
        "        else:\n",
        "            raise ValueError(\"Invalid Pauli letter\")\n",
        "    return sigma_prime, phase\n",
        "\n",
        "def local_estimator_sample(rbm: ComplexRBM, sigma, pauli_terms):\n",
        "    \"\"\"\n",
        "    pauli_terms: list of (coeff, pauli_word_list), e.g.\n",
        "      [(0.7, ['Z','I','X','X']), (0.3, ['I','Z','Z','I'])]\n",
        "    Returns O_loc(sigma) = sum_k c_k * <sigma|P_k|psi>/ <sigma|psi>\n",
        "    \"\"\"\n",
        "    logpsi_sigma = rbm.logpsi(sigma)\n",
        "    psi_sigma = np.exp(logpsi_sigma)\n",
        "    Oloc = 0.0 + 0.0j\n",
        "    for ck, Pk in pauli_terms:\n",
        "        sigma_k, phase_k = apply_pauli_on_config(sigma, Pk)\n",
        "        psi_sigma_k = rbm.psi(sigma_k)\n",
        "        Oloc += ck * phase_k * (psi_sigma_k / psi_sigma)\n",
        "    return Oloc\n",
        "\n",
        "def estimate_observable_mc(rbm: ComplexRBM, pauli_terms, n_samples=20000, burn_in=1000, thin=10):\n",
        "    samples = rbm.metropolis_samples(n_samples=n_samples, burn_in=burn_in, thin=thin)\n",
        "    vals = []\n",
        "    for s in samples:\n",
        "        vals.append(local_estimator_sample(rbm, s, pauli_terms))\n",
        "    vals = np.array(vals, dtype=np.complex128)\n",
        "    mean = np.mean(vals)\n",
        "    # Standard error from MC\n",
        "    stderr = np.std(vals)/np.sqrt(len(vals))\n",
        "    return mean, stderr\n",
        "\n",
        "# -------------------------------------------\n",
        "# Exact enumeration (small N) for validation\n",
        "# -------------------------------------------\n",
        "def all_spin_configs(N):\n",
        "    # Returns array of shape (2^N, N) with entries in {-1,+1}\n",
        "    arr = []\n",
        "    for idx in range(2**N):\n",
        "        bits = [((idx >> (N-1-i)) & 1) for i in range(N)]\n",
        "        sig = np.array([1 if b==0 else -1 for b in bits], dtype=int)  # 0->+1, 1->-1\n",
        "        arr.append(sig)\n",
        "    return np.stack(arr, axis=0)\n",
        "\n",
        "def exact_expectation(rbm: ComplexRBM, pauli_terms):\n",
        "    N = rbm.N\n",
        "    cfgs = all_spin_configs(N)\n",
        "    # Build normalized psi over all configs\n",
        "    psi = np.array([rbm.psi(s) for s in cfgs])\n",
        "    norm2 = np.vdot(psi, psi).real\n",
        "    psi /= np.sqrt(norm2)\n",
        "\n",
        "    # Compute <O> = sum_sigma |psi(sigma)|^2 O_loc(sigma)\n",
        "    Oexp = 0.0 + 0.0j\n",
        "    for sigma, psi_s in zip(cfgs, psi):\n",
        "        Oloc = 0.0 + 0.0j\n",
        "        for ck, Pk in pauli_terms:\n",
        "            sigma_k, phase_k = apply_pauli_on_config(sigma, Pk)\n",
        "            # find index of sigma_k\n",
        "            idx_k = 0\n",
        "            for i, s_i in enumerate(sigma_k):\n",
        "                bit = 0 if s_i==1 else 1\n",
        "                idx_k = (idx_k<<1) | bit\n",
        "            Oloc += ck * phase_k * (psi[idx_k] / psi_s)\n",
        "        Oexp += (np.abs(psi_s)**2) * Oloc\n",
        "    return Oexp\n",
        "\n",
        "# ----------------------------\n",
        "# Demo / self-test\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(0)\n",
        "    N = 3\n",
        "    M = 4\n",
        "    rbm = ComplexRBM(n_visible=N, n_hidden=M, rng=rng)  # <-- plug your trained params here\n",
        "\n",
        "    # Define an observable O = 0.8 * (X Z I) + 0.5 * (Z Z Z) - 0.3 * (Y I X)\n",
        "    pauli_terms = [\n",
        "        (0.8, ['X','Z','I']),\n",
        "        (0.5, ['Z','Z','Z']),\n",
        "        (-0.3, ['Y','I','X']),\n",
        "    ]\n",
        "\n",
        "    # Monte Carlo estimate from RBM samples\n",
        "    mc_mean, mc_se = estimate_observable_mc(\n",
        "        rbm, pauli_terms, n_samples=30000, burn_in=2000, thin=5\n",
        "    )\n",
        "    print(f\"MC estimate  <O> = {mc_mean:.6f}  ± {mc_se:.6f}  (standard error)\")\n",
        "\n",
        "    # Exact expectation from full enumeration (same RBM parameters)\n",
        "    exact = exact_expectation(rbm, pauli_terms)\n",
        "    print(f\"Exact value  <O> = {exact:.6f}\")\n",
        "\n",
        "    print(f\"Absolute error |MC - Exact| = {abs(mc_mean-exact):.6e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEI5HRO0xIey"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "nqs_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
