{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWy_Koj-t5CZ",
        "outputId": "489cf5cc-714e-4096-dab5-43ca16acbdff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empirical distribution from circuit (first 10 rows):\n",
            "[[1 1 0 1]\n",
            " [1 1 1 0]\n",
            " [1 0 0 0]\n",
            " [0 0 1 1]\n",
            " [1 1 0 1]\n",
            " [0 0 1 1]\n",
            " [1 0 1 1]\n",
            " [1 0 1 0]\n",
            " [1 1 1 0]\n",
            " [0 0 1 1]]\n",
            "Epoch  150  PLL≈ -2.7761\n",
            "Epoch  300  PLL≈ -2.7625\n",
            "Epoch  450  PLL≈ -2.7732\n",
            "Epoch  600  PLL≈ -2.7745\n",
            "Epoch  750  PLL≈ -2.7790\n",
            "Epoch  900  PLL≈ -2.7716\n",
            "Epoch 1050  PLL≈ -2.7687\n",
            "Epoch 1200  PLL≈ -2.7794\n",
            "Epoch 1350  PLL≈ -2.7743\n",
            "Epoch 1500  PLL≈ -2.7753\n",
            "\n",
            "--- Evaluation ---\n",
            "True probs (first 8):   [0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625]\n",
            "Model probs (first 8):  [0.0597 0.061  0.062  0.0619 0.0605 0.0609 0.0662 0.0626]\n",
            "TVD(true, model) = 0.0111\n",
            "KL(true || model) = 0.0004\n"
          ]
        }
      ],
      "source": [
        "# rbm_train_4q_test.py\n",
        "# Minimal, dependency-free (NumPy-only) test of training an RBM on bitstrings\n",
        "# sampled from a 4-qubit quantum circuit.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "def kron(*ops):\n",
        "    out = np.array([[1.0+0j]])\n",
        "    for op in ops:\n",
        "        out = np.kron(out, op)\n",
        "    return out\n",
        "\n",
        "def normalize(psi):\n",
        "    return psi / np.linalg.norm(psi)\n",
        "\n",
        "def sample_bitstrings_from_state(psi, n_samples, rng):\n",
        "    # psi is a 16-dim statevector for 4 qubits (|0000>..|1111> order)\n",
        "    probs = np.abs(psi)**2\n",
        "    idxs = rng.choice(len(probs), size=n_samples, p=probs)\n",
        "    # Convert indices to 4-bit strings (0/1)\n",
        "    bits = ((idxs[:, None] >> np.arange(4)[::-1]) & 1).astype(np.int64)\n",
        "    return bits\n",
        "\n",
        "def one_hot_counts(bits, n_qubits=4):\n",
        "    idx = np.packbits(bits, axis=1, bitorder='big')  # 4 bits -> 1 byte\n",
        "    # But packbits returns uint8 with all 8 bits; compute integer index manually:\n",
        "    idx = (bits[:,0]<<3) + (bits[:,1]<<2) + (bits[:,2]<<1) + (bits[:,3]<<0)\n",
        "    counts = np.bincount(idx, minlength=2**n_qubits)\n",
        "    return counts\n",
        "\n",
        "def total_variation_distance(p, q):\n",
        "    return 0.5 * np.sum(np.abs(p - q))\n",
        "\n",
        "# ---------- Simple 4-qubit circuit (statevector) ----------\n",
        "def four_qubit_state(rng=None):\n",
        "    # Gates\n",
        "    I = np.eye(2, dtype=complex)\n",
        "    X = np.array([[0,1],[1,0]], dtype=complex)\n",
        "    H = (1/np.sqrt(2))*np.array([[1,1],[1,-1]], dtype=complex)\n",
        "    Rz = lambda theta: np.array([[np.exp(-1j*theta/2),0],[0,np.exp(1j*theta/2)]], dtype=complex)\n",
        "    Rx = lambda theta: np.array([[np.cos(theta/2), -1j*np.sin(theta/2)],\n",
        "                                 [-1j*np.sin(theta/2), np.cos(theta/2)]], dtype=complex)\n",
        "    # CNOT on control c, target t; 4-qubit space\n",
        "    def CNOT(c, t):\n",
        "        # Build 16x16 operator by projecting on control\n",
        "        P0 = np.array([[1,0],[0,0]], dtype=complex)\n",
        "        P1 = np.array([[0,0],[0,1]], dtype=complex)\n",
        "        ops0 = [I,I,I,I]; ops1 = [I,I,I,I]\n",
        "        ops0[c] = P0; ops1[c] = P1\n",
        "        U0 = kron(*ops0)\n",
        "        # X on target when control is 1\n",
        "        opsx = [I,I,I,I]\n",
        "        opsx[t] = X\n",
        "        X_t = kron(*opsx)\n",
        "        return U0 + kron(*[P1 if i==c else I for i in range(4)]) @ X_t\n",
        "\n",
        "    # Start in |0000>\n",
        "    psi = np.zeros(16, dtype=complex); psi[0] = 1.0\n",
        "\n",
        "    # Layer 1: H on all qubits\n",
        "    U_H_all = kron(H,H,H,H)\n",
        "    psi = U_H_all @ psi\n",
        "\n",
        "    # Layer 2: random single-qubit rotations (fixed seed for reproducibility)\n",
        "    if rng is None: rng = np.random.default_rng(7)\n",
        "    thetas_rz = rng.uniform(-0.9, 0.9, size=4)\n",
        "    thetas_rx = rng.uniform(-0.9, 0.9, size=4)\n",
        "    U1 = kron(Rz(thetas_rz[0])@Rx(thetas_rx[0]),\n",
        "              Rz(thetas_rz[1])@Rx(thetas_rx[1]),\n",
        "              Rz(thetas_rz[2])@Rx(thetas_rx[2]),\n",
        "              Rz(thetas_rz[3])@Rx(thetas_rx[3]))\n",
        "    psi = U1 @ psi\n",
        "\n",
        "    # Layer 3: entanglers (CNOT 0->1 and 2->3)\n",
        "    psi = CNOT(0,1) @ psi\n",
        "    psi = CNOT(2,3) @ psi\n",
        "\n",
        "    return normalize(psi)\n",
        "\n",
        "# ---------- RBM (0/1 units) ----------\n",
        "class RBM01:\n",
        "    \"\"\"\n",
        "    RBM with 0/1 visible and hidden units.\n",
        "    Energy:  E(v,h) = -a^T v - b^T h - v^T W h\n",
        "    p(v,h) ∝ exp(-E). Conditionals:\n",
        "      p(h=1|v) = sigmoid(b + W^T v)\n",
        "      p(v=1|h) = sigmoid(a + W h)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_visible, n_hidden, rng=None, scale=0.01):\n",
        "        self.nv = n_visible\n",
        "        self.nh = n_hidden\n",
        "        self.rng = np.random.default_rng() if rng is None else rng\n",
        "        self.a = scale*self.rng.standard_normal(self.nv)\n",
        "        self.b = scale*self.rng.standard_normal(self.nh)\n",
        "        self.W = scale*self.rng.standard_normal((self.nv, self.nh))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "    def p_h_given_v(self, v):\n",
        "        # v: (B, nv) or (nv,)\n",
        "        x = self.b + v @ self.W\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "    def p_v_given_h(self, h):\n",
        "        x = self.a + h @ self.W.T\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        ph = self.p_h_given_v(v)\n",
        "        return (self.rng.random(ph.shape) < ph).astype(np.float64), ph\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        pv = self.p_v_given_h(h)\n",
        "        return (self.rng.random(pv.shape) < pv).astype(np.float64), pv\n",
        "\n",
        "    def cd_k(self, v0, k=1):\n",
        "        v = v0.copy()\n",
        "        h, ph = self.sample_h(v)\n",
        "        for _ in range(k):\n",
        "            v, pv = self.sample_v(h)\n",
        "            h, ph = self.sample_h(v)\n",
        "        return v, h  # vk, hk\n",
        "\n",
        "    def train(self, data, epochs=2000, batch_size=128, lr=0.05, k=1, verbose_every=200):\n",
        "        n = data.shape[0]\n",
        "        for ep in range(1, epochs+1):\n",
        "            # mini-batch SGD\n",
        "            perm = self.rng.permutation(n)\n",
        "            for i in range(0, n, batch_size):\n",
        "                batch = data[perm[i:i+batch_size]]\n",
        "                # Positive phase\n",
        "                ph_pos = self.p_h_given_v(batch)\n",
        "                # Negative phase via CD-k\n",
        "                v_k, h_k = self.cd_k(batch, k=k)\n",
        "                ph_neg = self.p_h_given_v(v_k)\n",
        "\n",
        "                # Gradients (expected sufficient statistics diff)\n",
        "                dW = (batch.T @ ph_pos - v_k.T @ ph_neg) / batch.shape[0]\n",
        "                da = (batch - v_k).mean(axis=0)\n",
        "                db = (ph_pos - ph_neg).mean(axis=0)\n",
        "\n",
        "                # Update\n",
        "                self.W += lr * dW\n",
        "                self.a += lr * da\n",
        "                self.b += lr * db\n",
        "\n",
        "            if verbose_every and ep % verbose_every == 0:\n",
        "                pll = self.pseudo_log_likelihood(data[:256])\n",
        "                print(f\"Epoch {ep:4d}  PLL≈ {pll:.4f}\")\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        # F(v) = -a^T v - sum_j log(1 + exp(b_j + W_j^T v))\n",
        "        lin = self.a @ v.T  # shape: (B,)\n",
        "        t = self.b + v @ self.W\n",
        "        logsum = np.sum(np.log1p(np.exp(t)), axis=1)\n",
        "        return -lin - logsum\n",
        "\n",
        "    def pseudo_log_likelihood(self, data):\n",
        "        # Standard RBM diagnostic: average PLL via single bit flip\n",
        "        B = min(256, data.shape[0])\n",
        "        idx = self.rng.integers(0, self.nv, size=B)\n",
        "        v = data[:B].copy()\n",
        "        fe_v = self.free_energy(v)\n",
        "        # flip selected bit\n",
        "        v_flipped = v.copy()\n",
        "        rows = np.arange(B)\n",
        "        v_flipped[rows, idx] = 1.0 - v_flipped[rows, idx]\n",
        "        fe_vf = self.free_energy(v_flipped)\n",
        "        pll = self.nv * np.mean(np.log(self.sigmoid(fe_vf - fe_v)))\n",
        "        return pll\n",
        "\n",
        "    def sample_model(self, n_samples=10000, k=20):\n",
        "        # Start from random visibles; run k-step block Gibbs; return samples\n",
        "        v = (self.rng.random((n_samples, self.nv)) < 0.5).astype(np.float64)\n",
        "        for _ in range(k):\n",
        "            h, _ = self.sample_h(v)\n",
        "            v, _ = self.sample_v(h)\n",
        "        return v.astype(int)\n",
        "\n",
        "# ---------- Main test ----------\n",
        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(123)\n",
        "\n",
        "    # 1) Build 4-qubit state and sample training data\n",
        "    psi = four_qubit_state(rng)\n",
        "    n_train = 20000\n",
        "    data_bits = sample_bitstrings_from_state(psi, n_train, rng)  # shape (n,4), 0/1\n",
        "    print(\"Empirical distribution from circuit (first 10 rows):\")\n",
        "    print(data_bits[:10])\n",
        "\n",
        "    # True distribution (for evaluation)\n",
        "    true_probs = np.abs(psi)**2  # length 16\n",
        "\n",
        "    # 2) Initialize and train RBM\n",
        "    rbm = RBM01(n_visible=4, n_hidden=8, rng=rng)\n",
        "    rbm.train(data_bits, epochs=1500, batch_size=128, lr=0.05, k=1, verbose_every=150)\n",
        "\n",
        "    # 3) Evaluate: sample from RBM model and compare to true probs\n",
        "    rbm_samples = rbm.sample_model(n_samples=200000, k=25)\n",
        "    model_counts = one_hot_counts(rbm_samples, n_qubits=4).astype(np.float64)\n",
        "    model_probs = model_counts / model_counts.sum()\n",
        "\n",
        "    tvd = total_variation_distance(true_probs, model_probs)\n",
        "    kl = np.sum(np.where(model_probs > 0, true_probs * (np.log(true_probs + 1e-12) - np.log(model_probs + 1e-12)), 0.0))\n",
        "    print(\"\\n--- Evaluation ---\")\n",
        "    print(\"True probs (first 8):  \", np.round(true_probs[:8], 4))\n",
        "    print(\"Model probs (first 8): \", np.round(model_probs[:8], 4))\n",
        "    print(f\"TVD(true, model) = {tvd:.4f}\")\n",
        "    print(f\"KL(true || model) = {kl:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1+1j)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1+1j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg9msoa_xHs0",
        "outputId": "4284f495-4ce8-4b5f-ace9-f32d4712939e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MC estimate  <O> = -0.023905+0.000677j  ± 0.005717  (standard error)\n",
            "Exact value  <O> = -0.018401+0.000000j\n",
            "Absolute error |MC - Exact| = 5.546242e-03\n"
          ]
        }
      ],
      "source": [
        "# rbm_observable_estimator_test.py\n",
        "# Estimate <O> for O = sum_k c_k P_k using a trained complex RBM psi_lambda.\n",
        "# Demonstrates MC local-estimator + exact enumeration cross-check (small N).\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------\n",
        "# Complex RBM wavefunction\n",
        "# ----------------------------\n",
        "class ComplexRBM:\n",
        "    \"\"\"\n",
        "    RBM with spins sigma_i in {-1, +1}.\n",
        "    psi(sigma) = exp(sum_i a_i sigma_i) * prod_j 2 cosh( b_j + sum_i W_ij sigma_i )\n",
        "    where a, b, W are complex.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_visible, n_hidden, a=None, b=None, W=None, rng=None):\n",
        "        self.N = n_visible\n",
        "        self.M = n_hidden\n",
        "        self.rng = np.random.default_rng() if rng is None else rng\n",
        "\n",
        "        # If not supplied, set arbitrary (random) complex params for demo\n",
        "        if a is None: a = 0.05*(self.rng.standard_normal(self.N) + 1j*self.rng.standard_normal(self.N))\n",
        "        if b is None: b = 0.05*(self.rng.standard_normal(self.M) + 1j*self.rng.standard_normal(self.M))\n",
        "        if W is None: W = 0.05*(self.rng.standard_normal((self.N,self.M)) + 1j*self.rng.standard_normal((self.N,self.M)))\n",
        "\n",
        "        self.a = a.astype(np.complex128)\n",
        "        self.b = b.astype(np.complex128)\n",
        "        self.W = W.astype(np.complex128)\n",
        "\n",
        "    def theta(self, sigma):\n",
        "        # theta_j = b_j + sum_i W_ij sigma_i ; shape (M,)\n",
        "        return self.b + (sigma @ self.W)\n",
        "\n",
        "    def logpsi(self, sigma):\n",
        "        # sigma: shape (N,) with entries in {-1, +1}\n",
        "        th = self.theta(sigma)  # (M,)\n",
        "        # log psi = sum_i a_i sigma_i + sum_j log(2 cosh(theta_j))\n",
        "        # Use np.cosh for complex; add small epsilon to avoid branch issues if desired\n",
        "        return np.sum(self.a * sigma) + np.sum(np.log(2.0*np.cosh(th)))\n",
        "\n",
        "    def psi(self, sigma):\n",
        "        return np.exp(self.logpsi(sigma))\n",
        "\n",
        "    # ---- Metropolis sampling from |psi|^2 ----\n",
        "    def metropolis_samples(self, n_samples=10000, burn_in=1000, thin=10):\n",
        "        sigma = self.random_spin_config()  # start\n",
        "        logpsi = self.logpsi(sigma)\n",
        "        # cache theta for O(M) updates\n",
        "        th = self.theta(sigma)\n",
        "\n",
        "        samples = []\n",
        "        total_steps = burn_in + n_samples*thin\n",
        "        for step in range(total_steps):\n",
        "            i = self.rng.integers(self.N)           # propose flip at i\n",
        "            sigma_new = sigma.copy()\n",
        "            sigma_new[i] *= -1\n",
        "\n",
        "            # Efficient theta' update: theta'_j = theta_j + W_ij (sigma'_i - sigma_i) = theta_j - 2 W_ij sigma_i\n",
        "            th_new = th - 2.0*self.W[i,:]*sigma[i]\n",
        "\n",
        "            # log |psi|^2 = 2 * Re(logpsi)\n",
        "            logpsi2 = 2.0*np.real(logpsi)\n",
        "            logpsi_new = (np.sum(self.a * sigma_new) + np.sum(np.log(2.0*np.cosh(th_new))))\n",
        "            logpsi2_new = 2.0*np.real(logpsi_new)\n",
        "\n",
        "            # Accept/reject\n",
        "            logR = logpsi2_new - logpsi2\n",
        "            if logR >= 0 or np.log(self.rng.random()) < logR:\n",
        "                sigma = sigma_new\n",
        "                logpsi = logpsi_new\n",
        "                th = th_new\n",
        "\n",
        "            # collect\n",
        "            if step >= burn_in and ((step - burn_in) % thin == 0):\n",
        "                samples.append(sigma.copy())\n",
        "\n",
        "        return np.array(samples, dtype=int)\n",
        "\n",
        "    def random_spin_config(self):\n",
        "        return 2*self.rng.integers(0,2,size=self.N)-1  # {-1,+1}\n",
        "\n",
        "# -------------------------------------------\n",
        "# Pauli words: apply in Z-basis & phase rules\n",
        "# -------------------------------------------\n",
        "# Represent a Pauli word as a list of ('I'/'X'/'Y'/'Z') of length N.\n",
        "# Local estimator for a single sample sigma uses:\n",
        "#   <sigma | P | psi> / <sigma | psi> = phase * psi(sigma_flipped)/psi(sigma)\n",
        "# where sigma_flipped flips bits where X or Y act; phase accumulates Z and Y phases.\n",
        "\n",
        "def apply_pauli_on_config(sigma, pauli_word):\n",
        "    \"\"\"\n",
        "    Returns (sigma_prime, phase) such that\n",
        "    <sigma | P | psi> = phase * psi(sigma_prime)\n",
        "    in the computational Z basis with sigma_i in {-1, +1} mapping to |0>, |1> by sigma_i=+1->|0|, -1->|1|.\n",
        "    Phase convention:\n",
        "      Z on |0>,|1> gives +1,-1 respectively => factor sigma_i\n",
        "      X flips the bit (no phase)\n",
        "      Y flips the bit and contributes phase i*(sign) where sign = -sigma_i (since Y|0>=i|1>, Y|1>=-i|0>)\n",
        "    \"\"\"\n",
        "    sigma_prime = sigma.copy()\n",
        "    phase = 1.0 + 0.0j\n",
        "    for i, P in enumerate(pauli_word):\n",
        "        if P == 'I':\n",
        "            continue\n",
        "        elif P == 'Z':\n",
        "            phase *= sigma[i]  # eigenvalue +1/-1\n",
        "        elif P == 'X':\n",
        "            sigma_prime[i] *= -1\n",
        "        elif P == 'Y':\n",
        "            # Y = i X Z ; acting on basis adds: flip and multiply by (i * sigma_i)\n",
        "            phase *= (1j * sigma[i])\n",
        "            sigma_prime[i] *= -1\n",
        "        else:\n",
        "            raise ValueError(\"Invalid Pauli letter\")\n",
        "    return sigma_prime, phase\n",
        "\n",
        "def local_estimator_sample(rbm: ComplexRBM, sigma, pauli_terms):\n",
        "    \"\"\"\n",
        "    pauli_terms: list of (coeff, pauli_word_list), e.g.\n",
        "      [(0.7, ['Z','I','X','X']), (0.3, ['I','Z','Z','I'])]\n",
        "    Returns O_loc(sigma) = sum_k c_k * <sigma|P_k|psi>/ <sigma|psi>\n",
        "    \"\"\"\n",
        "    logpsi_sigma = rbm.logpsi(sigma)\n",
        "    psi_sigma = np.exp(logpsi_sigma)\n",
        "    Oloc = 0.0 + 0.0j\n",
        "    for ck, Pk in pauli_terms:\n",
        "        sigma_k, phase_k = apply_pauli_on_config(sigma, Pk)\n",
        "        psi_sigma_k = rbm.psi(sigma_k)\n",
        "        Oloc += ck * phase_k * (psi_sigma_k / psi_sigma)\n",
        "    return Oloc\n",
        "\n",
        "def estimate_observable_mc(rbm: ComplexRBM, pauli_terms, n_samples=20000, burn_in=1000, thin=10):\n",
        "    samples = rbm.metropolis_samples(n_samples=n_samples, burn_in=burn_in, thin=thin)\n",
        "    vals = []\n",
        "    for s in samples:\n",
        "        vals.append(local_estimator_sample(rbm, s, pauli_terms))\n",
        "    vals = np.array(vals, dtype=np.complex128)\n",
        "    mean = np.mean(vals)\n",
        "    # Standard error from MC\n",
        "    stderr = np.std(vals)/np.sqrt(len(vals))\n",
        "    return mean, stderr\n",
        "\n",
        "# -------------------------------------------\n",
        "# Exact enumeration (small N) for validation\n",
        "# -------------------------------------------\n",
        "def all_spin_configs(N):\n",
        "    # Returns array of shape (2^N, N) with entries in {-1,+1}\n",
        "    arr = []\n",
        "    for idx in range(2**N):\n",
        "        bits = [((idx >> (N-1-i)) & 1) for i in range(N)]\n",
        "        sig = np.array([1 if b==0 else -1 for b in bits], dtype=int)  # 0->+1, 1->-1\n",
        "        arr.append(sig)\n",
        "    return np.stack(arr, axis=0)\n",
        "\n",
        "# def exact_expectation(rbm: ComplexRBM, pauli_terms):\n",
        "#     N = rbm.N\n",
        "#     cfgs = all_spin_configs(N)\n",
        "#     # Build normalized psi over all configs\n",
        "#     psi = np.array([rbm.psi(s) for s in cfgs])\n",
        "#     norm2 = np.vdot(psi, psi).real\n",
        "#     psi /= np.sqrt(norm2)\n",
        "\n",
        "#     # Compute <O> = sum_sigma |psi(sigma)|^2 O_loc(sigma)\n",
        "#     Oexp = 0.0 + 0.0j\n",
        "#     for sigma, psi_s in zip(cfgs, psi):\n",
        "#         Oloc = 0.0 + 0.0j\n",
        "#         for ck, Pk in pauli_terms:\n",
        "#             sigma_k, phase_k = apply_pauli_on_config(sigma, Pk)\n",
        "#             # find index of sigma_k\n",
        "#             idx_k = 0\n",
        "#             for i, s_i in enumerate(sigma_k):\n",
        "#                 bit = 0 if s_i==1 else 1\n",
        "#                 idx_k = (idx_k<<1) | bit\n",
        "#             Oloc += ck * phase_k * (psi[idx_k] / psi_s)\n",
        "#         Oexp += (np.abs(psi_s)**2) * Oloc\n",
        "#     return Oexp\n",
        "\n",
        "def exact_expectation(rbm: ComplexRBM, pauli_terms):\n",
        "    N = rbm.N\n",
        "    cfgs = all_spin_configs(N)\n",
        "    # Build normalized psi over all configs\n",
        "    psi = np.array([rbm.psi(s) for s in cfgs])\n",
        "    norm2 = np.vdot(psi, psi).real\n",
        "    psi /= np.sqrt(norm2)\n",
        "    \n",
        "    # Create a dictionary for O(1) lookups\n",
        "    cfg_to_idx = {tuple(sigma): i for i, sigma in enumerate(cfgs)}\n",
        "\n",
        "    # Compute <O> = sum_sigma |psi(sigma)|^2 O_loc(sigma)\n",
        "    Oexp = 0.0 + 0.0j\n",
        "    for idx, (sigma, psi_s) in enumerate(zip(cfgs, psi)):\n",
        "        Oloc = 0.0 + 0.0j\n",
        "        for ck, Pk in pauli_terms:\n",
        "            sigma_k, phase_k = apply_pauli_on_config(sigma, Pk)\n",
        "            idx_k = cfg_to_idx[tuple(sigma_k)]\n",
        "            Oloc += ck * phase_k * (psi[idx_k] / psi_s)\n",
        "        Oexp += (np.abs(psi_s)**2) * Oloc\n",
        "    return Oexp\n",
        "\n",
        "# ----------------------------\n",
        "# Demo / self-test\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(0)\n",
        "    N = 3\n",
        "    M = 4\n",
        "    rbm = ComplexRBM(n_visible=N, n_hidden=M, rng=rng)  # <-- plug your trained params here\n",
        "\n",
        "    # Define an observable O = 0.8 * (X Z I) + 0.5 * (Z Z Z) - 0.3 * (Y I X)\n",
        "    pauli_terms = [\n",
        "        (0.8, ['X','Z','I']),\n",
        "        (0.5, ['Z','Z','Z']),\n",
        "        (-0.3, ['Y','I','X']),\n",
        "    ]\n",
        "\n",
        "    # Monte Carlo estimate from RBM samples\n",
        "    mc_mean, mc_se = estimate_observable_mc(\n",
        "        rbm, pauli_terms, n_samples=30000, burn_in=2000, thin=5\n",
        "    )\n",
        "    print(f\"MC estimate  <O> = {mc_mean:.6f}  ± {mc_se:.6f}  (standard error)\")\n",
        "\n",
        "    # Exact expectation from full enumeration (same RBM parameters)\n",
        "    exact = exact_expectation(rbm, pauli_terms)\n",
        "    print(f\"Exact value  <O> = {exact:.6f}\")\n",
        "\n",
        "    print(f\"Absolute error |MC - Exact| = {abs(mc_mean-exact):.6e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ComplexRBM...\n",
            "Epoch  100  Avg log|psi|^2 ≈ 11.2453\n",
            "Epoch  200  Avg log|psi|^2 ≈ 11.2665\n",
            "Epoch  300  Avg log|psi|^2 ≈ 11.3106\n",
            "Epoch  400  Avg log|psi|^2 ≈ 11.2942\n",
            "Epoch  500  Avg log|psi|^2 ≈ 11.2984\n",
            "Epoch  600  Avg log|psi|^2 ≈ 11.2769\n",
            "Epoch  700  Avg log|psi|^2 ≈ 11.2859\n",
            "Epoch  800  Avg log|psi|^2 ≈ 11.2851\n",
            "Epoch  900  Avg log|psi|^2 ≈ 11.3377\n",
            "Epoch 1000  Avg log|psi|^2 ≈ 11.3424\n",
            "\n",
            "Training complete!\n",
            "Final parameters: a.shape=(6,), b.shape=(8,), W.shape=(6, 8)\n",
            "\n",
            "Sampling from trained model...\n",
            "Generated 1000 samples\n",
            "First 5 samples (in {-1,+1} format):\n",
            "[[1 1 1 1 1 0]\n",
            " [1 0 0 1 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 0 1 1 0 1]\n",
            " [0 0 0 1 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# complex_rbm_training.py\n",
        "# Training code for ComplexRBM with spins in {-1, +1}\n",
        "# Adapted to work with the ComplexRBM class from the observable estimator\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class ComplexRBM:\n",
        "    \"\"\"\n",
        "    RBM with spins sigma_i in {-1, +1}.\n",
        "    psi(sigma) = exp(sum_i a_i sigma_i) * prod_j 2 cosh( b_j + sum_i W_ij sigma_i )\n",
        "    where a, b, W are complex.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_visible, n_hidden, a=None, b=None, W=None, rng=None):\n",
        "        self.N = n_visible\n",
        "        self.M = n_hidden\n",
        "        self.rng = np.random.default_rng() if rng is None else rng\n",
        "\n",
        "        # If not supplied, initialize with small random complex parameters\n",
        "        if a is None: \n",
        "            a = 0.01*(self.rng.standard_normal(self.N) + 1j*self.rng.standard_normal(self.N))\n",
        "        if b is None: \n",
        "            b = 0.01*(self.rng.standard_normal(self.M) + 1j*self.rng.standard_normal(self.M))\n",
        "        if W is None: \n",
        "            W = 0.01*(self.rng.standard_normal((self.N,self.M)) + 1j*self.rng.standard_normal((self.N,self.M)))\n",
        "\n",
        "        self.a = a.astype(np.complex128)\n",
        "        self.b = b.astype(np.complex128)\n",
        "        self.W = W.astype(np.complex128)\n",
        "\n",
        "    def theta(self, sigma):\n",
        "        \"\"\"theta_j = b_j + sum_i W_ij sigma_i ; shape (..., M)\"\"\"\n",
        "        return self.b + (sigma @ self.W)\n",
        "\n",
        "    def logpsi(self, sigma):\n",
        "        \"\"\"Log wavefunction amplitude for configuration(s) sigma\"\"\"\n",
        "        # sigma: shape (..., N) with entries in {-1, +1}\n",
        "        th = self.theta(sigma)  # (..., M)\n",
        "        return np.sum(self.a * sigma, axis=-1) + np.sum(np.log(2.0*np.cosh(th)), axis=-1)\n",
        "\n",
        "    def psi(self, sigma):\n",
        "        \"\"\"Wavefunction amplitude\"\"\"\n",
        "        return np.exp(self.logpsi(sigma))\n",
        "\n",
        "    def prob(self, sigma):\n",
        "        \"\"\"Probability |psi(sigma)|^2 (unnormalized)\"\"\"\n",
        "        logpsi_val = self.logpsi(sigma)\n",
        "        return np.exp(2.0 * np.real(logpsi_val))\n",
        "\n",
        "    def sample_h_given_v(self, sigma):\n",
        "        \"\"\"\n",
        "        Sample hidden units given visible spins.\n",
        "        For complex RBM: p(h_j = ±1 | sigma) ∝ exp(±Re[b_j + sum_i W_ij sigma_i])\n",
        "        \"\"\"\n",
        "        th = self.theta(sigma)  # (..., M)\n",
        "        # p(h=+1) / p(h=-1) = exp(2*Re(theta))\n",
        "        # p(h=+1) = exp(Re(theta)) / (exp(Re(theta)) + exp(-Re(theta))) = 1/(1+exp(-2*Re(theta)))\n",
        "        prob_plus = 1.0 / (1.0 + np.exp(-2.0 * np.real(th)))\n",
        "        h = np.where(self.rng.random(th.shape) < prob_plus, 1, -1)\n",
        "        return h\n",
        "\n",
        "    def sample_v_given_h(self, h):\n",
        "        \"\"\"\n",
        "        Sample visible spins given hidden units.\n",
        "        For complex RBM: p(sigma_i = ±1 | h) ∝ exp(±Re[a_i + sum_j W_ij h_j])\n",
        "        \"\"\"\n",
        "        field = self.a + (h @ self.W.T)  # (..., N)\n",
        "        prob_plus = 1.0 / (1.0 + np.exp(-2.0 * np.real(field)))\n",
        "        sigma = np.where(self.rng.random(field.shape) < prob_plus, 1, -1)\n",
        "        return sigma\n",
        "\n",
        "    def contrastive_divergence_k(self, sigma0, k=1):\n",
        "        \"\"\"\n",
        "        CD-k: Start from data sigma0, run k steps of Gibbs sampling.\n",
        "        Returns final visible and hidden samples.\n",
        "        \"\"\"\n",
        "        sigma = sigma0.copy()\n",
        "        for _ in range(k):\n",
        "            h = self.sample_h_given_v(sigma)\n",
        "            sigma = self.sample_v_given_h(h)\n",
        "        h = self.sample_h_given_v(sigma)\n",
        "        return sigma, h\n",
        "\n",
        "    def compute_gradients(self, data_batch, k=1):\n",
        "        \"\"\"\n",
        "        Compute gradients using contrastive divergence.\n",
        "        Returns: grad_a, grad_b, grad_W\n",
        "        \"\"\"\n",
        "        batch_size = data_batch.shape[0]\n",
        "        \n",
        "        # Positive phase: data statistics\n",
        "        th_pos = self.theta(data_batch)  # (B, M)\n",
        "        h_pos_mean = np.tanh(np.real(th_pos))  # <h_j>_data ≈ tanh(Re(theta))\n",
        "        \n",
        "        # Negative phase: model statistics via CD-k\n",
        "        sigma_neg, h_neg = self.contrastive_divergence_k(data_batch, k=k)\n",
        "        th_neg = self.theta(sigma_neg)\n",
        "        h_neg_mean = np.tanh(np.real(th_neg))\n",
        "        \n",
        "        # Gradients for complex parameters (we optimize Re and Im separately via complex gradient)\n",
        "        # For complex params, gradient is: ∂L/∂θ* (conjugate)\n",
        "        # But for simplicity, we'll update using real gradients on log|psi|^2\n",
        "        \n",
        "        # Gradient of log|psi|^2 w.r.t. complex parameters\n",
        "        # d log|psi|^2 / da* = 2 * sigma * psi/|psi|^2 ≈ 2 * sigma (in expectation)\n",
        "        grad_a = np.mean(data_batch, axis=0) - np.mean(sigma_neg, axis=0)\n",
        "        \n",
        "        # d log|psi|^2 / db* involves tanh terms\n",
        "        grad_b = np.mean(h_pos_mean, axis=0) - np.mean(h_neg_mean, axis=0)\n",
        "        \n",
        "        # d log|psi|^2 / dW*\n",
        "        grad_W = (data_batch.T @ h_pos_mean - sigma_neg.T @ h_neg_mean) / batch_size\n",
        "        \n",
        "        return grad_a, grad_b, grad_W\n",
        "\n",
        "    def train(self, data_01, epochs=2000, batch_size=128, lr=0.05, k=1, verbose_every=200):\n",
        "        \"\"\"\n",
        "        Train the ComplexRBM on data.\n",
        "        \n",
        "        Args:\n",
        "            data_01: Training data in {0, 1} format, shape (n_samples, n_visible)\n",
        "            epochs: Number of training epochs\n",
        "            batch_size: Mini-batch size\n",
        "            lr: Learning rate\n",
        "            k: Number of CD steps\n",
        "            verbose_every: Print progress every N epochs (None to disable)\n",
        "        \"\"\"\n",
        "        # Convert 0/1 data to -1/+1 spins\n",
        "        data_spins = 2 * data_01 - 1  # 0 -> -1, 1 -> +1\n",
        "        n = data_spins.shape[0]\n",
        "        \n",
        "        for ep in range(1, epochs + 1):\n",
        "            # Shuffle data\n",
        "            perm = self.rng.permutation(n)\n",
        "            \n",
        "            # Mini-batch SGD\n",
        "            for i in range(0, n, batch_size):\n",
        "                batch = data_spins[perm[i:i+batch_size]]\n",
        "                \n",
        "                # Compute gradients\n",
        "                grad_a, grad_b, grad_W = self.compute_gradients(batch, k=k)\n",
        "                \n",
        "                # Update parameters (complex gradient descent)\n",
        "                # We treat complex params as having real and imaginary parts\n",
        "                # For simplicity, update both parts with the same gradient\n",
        "                self.a += lr * grad_a.astype(np.complex128)\n",
        "                self.b += lr * grad_b.astype(np.complex128)\n",
        "                self.W += lr * grad_W.astype(np.complex128)\n",
        "            \n",
        "            # Verbose output\n",
        "            if verbose_every and ep % verbose_every == 0:\n",
        "                # Compute approximate log-likelihood on a subset\n",
        "                sample_size = min(256, n)\n",
        "                sample_idx = self.rng.choice(n, size=sample_size, replace=False)\n",
        "                sample_data = data_spins[sample_idx]\n",
        "                avg_logprob = np.mean(2.0 * np.real(self.logpsi(sample_data)))\n",
        "                print(f\"Epoch {ep:4d}  Avg log|psi|^2 ≈ {avg_logprob:.4f}\")\n",
        "\n",
        "    def metropolis_samples(self, n_samples=10000, burn_in=1000, thin=10, return_format='bits'):\n",
        "        \"\"\"\n",
        "        Sample from |psi|^2 using Metropolis-Hastings\n",
        "        \n",
        "        Args:\n",
        "            n_samples: Number of samples to return\n",
        "            burn_in: Number of initial steps to discard\n",
        "            thin: Keep every thin-th sample\n",
        "            return_format: 'spins' for {-1,+1} or 'bits' for {0,1}\n",
        "        \n",
        "        Returns:\n",
        "            Array of samples in requested format\n",
        "        \"\"\"\n",
        "        sigma = self.random_spin_config()\n",
        "        logpsi = self.logpsi(sigma)\n",
        "        th = self.theta(sigma)\n",
        "\n",
        "        samples = []\n",
        "        total_steps = burn_in + n_samples * thin\n",
        "        \n",
        "        for step in range(total_steps):\n",
        "            # Propose single spin flip\n",
        "            i = self.rng.integers(self.N)\n",
        "            sigma_new = sigma.copy()\n",
        "            sigma_new[i] *= -1\n",
        "\n",
        "            # Efficient theta update\n",
        "            th_new = th - 2.0 * self.W[i, :] * sigma[i]\n",
        "\n",
        "            # Metropolis acceptance\n",
        "            logpsi_new = np.sum(self.a * sigma_new) + np.sum(np.log(2.0*np.cosh(th_new)))\n",
        "            logpsi2 = 2.0 * np.real(logpsi)\n",
        "            logpsi2_new = 2.0 * np.real(logpsi_new)\n",
        "            \n",
        "            logR = logpsi2_new - logpsi2\n",
        "            if logR >= 0 or np.log(self.rng.random()) < logR:\n",
        "                sigma = sigma_new\n",
        "                logpsi = logpsi_new\n",
        "                th = th_new\n",
        "\n",
        "            # Collect sample\n",
        "            if step >= burn_in and ((step - burn_in) % thin == 0):\n",
        "                samples.append(sigma.copy())\n",
        "\n",
        "        samples = np.array(samples, dtype=int)\n",
        "        \n",
        "        # Convert to bits if requested\n",
        "        if return_format == 'bits':\n",
        "            samples = ((samples + 1) // 2).astype(int)  # -1 -> 0, +1 -> 1\n",
        "        \n",
        "        return samples\n",
        "\n",
        "\n",
        "    def random_spin_config(self):\n",
        "        \"\"\"Generate random spin configuration in {-1, +1}\"\"\"\n",
        "        return 2 * self.rng.integers(0, 2, size=self.N) - 1\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: Train on 6-qubit data\n",
        "    rng = np.random.default_rng(42)\n",
        "    \n",
        "    # Generate some example training data (0/1 format)\n",
        "    n_train = 5000\n",
        "    n_qubits = 6\n",
        "    training_data = rng.integers(0, 2, size=(n_train, n_qubits)).astype(float)\n",
        "    \n",
        "    # Create and train ComplexRBM\n",
        "    rbm = ComplexRBM(n_visible=n_qubits, n_hidden=8, rng=rng)\n",
        "    \n",
        "    print(\"Training ComplexRBM...\")\n",
        "    rbm.train(training_data, epochs=1000, batch_size=128, lr=0.05, k=1, verbose_every=100)\n",
        "    \n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(f\"Final parameters: a.shape={rbm.a.shape}, b.shape={rbm.b.shape}, W.shape={rbm.W.shape}\")\n",
        "    \n",
        "    # Sample from trained model\n",
        "    print(\"\\nSampling from trained model...\")\n",
        "    samples = rbm.metropolis_samples(n_samples=1000, burn_in=500, thin=5)\n",
        "    print(f\"Generated {len(samples)} samples\")\n",
        "    print(\"First 5 samples (in {-1,+1} format):\")\n",
        "    print(samples[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yEI5HRO0xIey"
      },
      "outputs": [],
      "source": [
        " # BEH2\n",
        " \n",
        "pauli_path = \"../data/tomography/BeH2/paulis.file\"\n",
        "interactions_path = \"../data/tomography/BeH2/interactions.file\"\n",
        "\n",
        "pauli = np.load(pauli_path,allow_pickle=True)                                 \n",
        "interactions = np.load(interactions_path,allow_pickle=True)                   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nqs.BeH2_ptutorial as bpm\n",
        "import netket as nk\n",
        "\n",
        "hi = nk.hilbert.Qubit(N=6)\n",
        "samples_path = \"../data/tomography/BeH2/train_samples.txt\"\n",
        "bases_path = \"../data/tomography/BeH2/train_bases.txt\"\n",
        "ns = 100000  \n",
        "N_beh2=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 128000 measurement bases from ../data/tomography/BeH2/train_bases.txt\n",
            "Number of qubits: 6\n",
            "Unique bases: 54\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load training data\n",
        "rotations, tr_samples, tr_bases = bpm.LoadData(N_beh2,hi,samples_path, bases_path)\n",
        "if (ns > tr_samples.shape[0]):\n",
        "    \"Not enough training samples\"\n",
        "else:\n",
        "    training_samples = tr_samples[0:ns]\n",
        "    training_bases   = tr_bases[0:ns]\n",
        "    \n",
        "# load bases using new function:\n",
        "Us = bpm.load_bases(bases_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 1., 0., 1.],\n",
              "       [0., 0., 1., 0., 1., 1.],\n",
              "       [1., 1., 0., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 0., 1.],\n",
              "       [0., 0., 0., 0., 1., 1.],\n",
              "       [0., 0., 1., 0., 1., 1.],\n",
              "       [0., 1., 1., 0., 0., 1.],\n",
              "       [1., 1., 1., 1., 1., 0.],\n",
              "       [0., 1., 0., 1., 0., 1.],\n",
              "       [0., 0., 1., 1., 0., 0.]])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tr_samples[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_pauli_format(pauli_strings, coefficients):\n",
        "    \"\"\"\n",
        "    Convert Pauli strings from string format to list format with coefficients.\n",
        "    \n",
        "    Args:\n",
        "        pauli_strings: List of strings like ['IIIIII', 'ZIIIII', ...]\n",
        "        coefficients: List of floats corresponding to each Pauli string\n",
        "    \n",
        "    Returns:\n",
        "        List of tuples: [(coeff, ['I','I','I',...]), ...]\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for pauli_str, coeff in zip(pauli_strings, coefficients):\n",
        "        pauli_list = list(pauli_str)\n",
        "        result.append((coeff, pauli_list))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "converted_strs = convert_pauli_format(pauli,interactions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(np.float64(-17.07279443954866), ['I', 'I', 'I', 'I', 'I', 'I']), (np.float64(0.11563387473407918), ['Z', 'I', 'I', 'I', 'I', 'I']), (np.float64(0.007269543961396736), ['X', 'X', 'Z', 'I', 'I', 'I']), (np.float64(0.007269543961396736), ['Y', 'Y', 'I', 'I', 'I', 'I']), (np.float64(0.10000450328928216), ['Z', 'Z', 'I', 'I', 'I', 'I']), (np.float64(-0.004618457080023506), ['Z', 'X', 'X', 'I', 'I', 'I']), (np.float64(-0.004618457080023506), ['I', 'Y', 'Y', 'I', 'I', 'I']), (np.float64(-0.15987964617217187), ['I', 'Z', 'Z', 'I', 'I', 'I']), (np.float64(-0.3907728921618504), ['I', 'I', 'Z', 'I', 'I', 'I']), (np.float64(0.1156338747340795), ['I', 'I', 'I', 'Z', 'I', 'I'])]\n"
          ]
        }
      ],
      "source": [
        "print(converted_strs[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   50  Avg log|psi|^2 ≈ 14.9880\n",
            "Epoch  100  Avg log|psi|^2 ≈ 14.9845\n",
            "Epoch  150  Avg log|psi|^2 ≈ 15.0610\n",
            "Epoch  200  Avg log|psi|^2 ≈ 15.0113\n",
            "Epoch  250  Avg log|psi|^2 ≈ 15.0174\n",
            "Epoch  300  Avg log|psi|^2 ≈ 15.3712\n",
            "Epoch  350  Avg log|psi|^2 ≈ 15.7565\n",
            "Epoch  400  Avg log|psi|^2 ≈ 15.5838\n",
            "Epoch  450  Avg log|psi|^2 ≈ 15.6272\n",
            "Epoch  500  Avg log|psi|^2 ≈ 15.5002\n",
            "Epoch  550  Avg log|psi|^2 ≈ 15.4099\n",
            "Epoch  600  Avg log|psi|^2 ≈ 15.4887\n",
            "Epoch  650  Avg log|psi|^2 ≈ 15.7770\n",
            "Epoch  700  Avg log|psi|^2 ≈ 15.7499\n",
            "Epoch  750  Avg log|psi|^2 ≈ 15.5934\n",
            "Epoch  800  Avg log|psi|^2 ≈ 15.8502\n",
            "Epoch  850  Avg log|psi|^2 ≈ 15.6710\n",
            "Epoch  900  Avg log|psi|^2 ≈ 15.6929\n",
            "Epoch  950  Avg log|psi|^2 ≈ 15.5122\n",
            "Epoch 1000  Avg log|psi|^2 ≈ 15.6946\n",
            "Epoch 1050  Avg log|psi|^2 ≈ 15.9296\n",
            "Epoch 1100  Avg log|psi|^2 ≈ 15.6452\n",
            "Epoch 1150  Avg log|psi|^2 ≈ 15.8675\n",
            "Epoch 1200  Avg log|psi|^2 ≈ 15.9492\n",
            "Epoch 1250  Avg log|psi|^2 ≈ 15.7281\n",
            "Epoch 1300  Avg log|psi|^2 ≈ 15.9149\n",
            "Epoch 1350  Avg log|psi|^2 ≈ 16.3465\n",
            "Epoch 1400  Avg log|psi|^2 ≈ 16.3576\n",
            "Epoch 1450  Avg log|psi|^2 ≈ 16.0587\n",
            "Epoch 1500  Avg log|psi|^2 ≈ 16.0595\n"
          ]
        }
      ],
      "source": [
        "# 2) Initialize and train RBM\n",
        "rbm_beh2 = ComplexRBM(n_visible=6, n_hidden=8, rng=rng)\n",
        "rbm_beh2.train(tr_samples[:ns], epochs=1500, batch_size=10000, lr=0.01, k=1, verbose_every=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MC estimate  <O> = -16.676977-0.000467j  ± 0.000509  (standard error)\n",
            "Exact value  <O> = -16.733263-0.000000j\n",
            "Absolute error |MC - Exact| = 5.628807e-02\n"
          ]
        }
      ],
      "source": [
        "# Monte Carlo estimate from RBM samples\n",
        "beh2_mean, beh2_se = estimate_observable_mc(\n",
        "    rbm_beh2, converted_strs, n_samples=100000, burn_in=2000, thin=5\n",
        ")\n",
        "print(f\"MC estimate  <O> = {beh2_mean:.6f}  ± {beh2_se:.6f}  (standard error)\")\n",
        "\n",
        "# Exact expectation from full enumeration (same RBM parameters)\n",
        "exact_beh2 = exact_expectation(rbm_beh2, converted_strs)\n",
        "print(f\"Exact value  <O> = {exact_beh2:.6f}\")\n",
        "\n",
        "print(f\"Absolute error |MC - Exact| = {abs(beh2_mean-exact_beh2):.6e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "nqs_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
